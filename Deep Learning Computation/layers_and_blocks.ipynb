{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers and Blocks\n",
    "\n",
    "When we first started talking about neural networks,\n",
    "we introduced linear models with a single output.\n",
    "Here, the entire model consists of just a single neuron.\n",
    "By itself, a single neuron takes some set of inputs,\n",
    "generates a corresponding (*scalar*) output,\n",
    "and has a set of associated parameters that can be updated\n",
    "to optimize some objective function of interest.\n",
    "Then, once we started thinking about networks with multiple outputs,\n",
    "we leveraged vectorized arithmetic,\n",
    "we showed how we could use linear algebra\n",
    "to efficiently express an entire *layer* of neurons.\n",
    "Layers too expect some inputs, generate corresponding outputs,\n",
    "and are described by a set of tunable parameters.\n",
    "\n",
    "When we worked through softmax regression,\n",
    "a single *layer* was itself *the model*.\n",
    "However, when we subsequently introduced multilayer perceptrons,\n",
    "we developed models consisting of multiple layers.\n",
    "One interesting property of multilayer neural networks\n",
    "is that the *entire model* and its *constituent layers*\n",
    "share the same basic structure.\n",
    "The model takes the true inputs (as stated in the problem formulation),\n",
    "outputs predictions of the true outputs,\n",
    "and possesses parameters (the combined set of all parameters from all layers)\n",
    "Likewise any individual constituent layer in a multilayer perceptron\n",
    "ingests inputs (supplied by the previous layer)\n",
    "generates outputs (which form the inputs to the subsequent layer),\n",
    "and possesses a set of tunable parameters\n",
    "tht are updated with respect to the ultimate objective\n",
    "(using the signal that flows backwards through the subsequent layer).\n",
    "\n",
    "While you might think that neurons, layers, and models\n",
    "give us enough abstractions to go about our business,\n",
    "it turns out that we will often want to express our model\n",
    "in terms of a components that are large than an indivudal layer.\n",
    "For example, when designing models, like ResNet-152,\n",
    "which possess hundreds (152, thus the name) of layers,\n",
    "implementing the network one layer at a time can grow tedious.\n",
    "Moreover, this concern is not just hypothetical---such deep networks\n",
    "dominate numerous application areas, especially when training data is abundant.\n",
    "For example the ResNet architecture mentioned above\n",
    "won the 2015 ImageNet and COCO computer vision competitions\n",
    "for both recognition and detection.\n",
    "Deep networks with many layers arranged into components\n",
    "with various repeating patterns are now ubiquitous in other domains\n",
    "including natural language processing and speech.\n",
    "\n",
    "To facilitate the implementation of networks consisting of components\n",
    "of arbitrary complexity, we introduce a new flexible concept:\n",
    "a neural network *block*.\n",
    "A block could describe a single neuron,\n",
    "a high-dimensional layer,\n",
    "or an arbitrarily-complex component consisting of multiple layers.\n",
    "From a software development, a `Block` is a class.\n",
    "Any subclass of `Block` must define a method called `forward`\n",
    "that transforms its input into output,\n",
    "and must store any necessary parameters.\n",
    "Note that some Blocks do not require any parameters at all!\n",
    "Finally a `Block` must possess a `backward` method,\n",
    "for purposes of calculating gradients.\n",
    "Fortunately, due to some behind-the-scenes magic\n",
    "supplied by the autograd  `autograd` package\n",
    "(introduced in `chap_preliminaries`)\n",
    "when defining our own `Block` typically requires\n",
    "only that we worry about parameters and the `forward` function.\n",
    "\n",
    "\n",
    "One benefit of working with the `Block` abstraction is that\n",
    "they can be combined into larger artifacts, often recursively,\n",
    "e.g., as illustrated in `fig_blocks`.\n",
    "\n",
    "![Multiple layers are combined into blocks](../img/blocks.svg)\n",
    "\n",
    "By defining code to generate Blocks of arbitrary complexity on demand,\n",
    "we can write surprisingly compact code\n",
    "and still implement complex neural networks.\n",
    "\n",
    "To begin, we revisit the Blocks that played a role\n",
    "in our implementation of the multilayer perceptron (`sec_mlp_gluon`).\n",
    "The following code generates a network\n",
    "with one fully-connected hidden layer containing 256 units\n",
    "followed by a ReLU activation,\n",
    "and then another fully-connected layer\n",
    "consisting of 10 units (with no activation function).\n",
    "Because there are no more layers,\n",
    "this last 10-unit layer is regarded as the *output layer*\n",
    "and its outputs are also the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06240272, -0.03268593,  0.02582653,  0.02254182, -0.03728798,\n",
       "        -0.04253786,  0.00540613, -0.01364186, -0.09915452, -0.02272738],\n",
       "       [ 0.02816677, -0.03341204,  0.03565666,  0.02506382, -0.04136416,\n",
       "        -0.04941845,  0.01738528,  0.01081961, -0.09932579, -0.01176298]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet import np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "x = np.random.uniform(size=(2, 20))\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, as in previous chapters,\n",
    "our model consists of an object returned by the `nn.Sequential` constructor.\n",
    "After instantiating a `nn.Sequential` and storing the `net` variable,\n",
    "we repeatedly called its `add` method,\n",
    "appending layers in the order that they should be executed.\n",
    "We suspect that you might have already understood *more or less*\n",
    "what was going on here the first time you saw this code.\n",
    "You may even have understood it well enough\n",
    "to modify the code and design your own networks.\n",
    "However, the details regarding\n",
    "what exactly happens inside `nn.Sequential`\n",
    "have remained mysterious so far.\n",
    "\n",
    "In short, `nn.Sequential` just defines a special kind of Block.\n",
    "Specifically, an `nn.Sequential` maintains a list of constituent `Blocks`,\n",
    "stored in a particular order.\n",
    "You might think of `nnSequential` as your first meta-Block.\n",
    "The `add` method simply facilitates\n",
    "the addition of each successive `Block` to the list.\n",
    "Note that each our layers are instances of the `Dense` class\n",
    "which is itself a subclass of `Block`.\n",
    "The `forward` function is also remarkably simple:\n",
    "it chains each Block in the list together,\n",
    "passing the output of each as the input to the next.\n",
    "\n",
    "Note that until now, we have been invoking our models\n",
    "via the construction `net(X)` to obtain their outputs.\n",
    "This is actually just shorthand for `net.forward(X)`,\n",
    "a slick Python trick achieved via the Block class's `__call__` function.\n",
    "\n",
    "\n",
    "Before we dive in to implementing our own custom `Block`,\n",
    "we briefly summarize the basic functionality that each `Block` must perform the following duties:\n",
    "\n",
    "1. Ingest input data as arguments to its `forward` function.\n",
    "1. Generate an output via the value returned by its `forward` function. Note that the output may have a different shape from the input. For example, the first Dense layer in our model above ingests an input of arbitrary dimension but returns an output of dimension 256.\n",
    "1. Calculate the gradient of its output with respect to its input, which can be accessed via its `backward` method. Typically this happens automatically.\n",
    "1. Store and provide access to those parameters necessary to execute the `forward` computation.\n",
    "1. Initialize these parameters as needed.\n",
    "\n",
    "## A Custom Block\n",
    "\n",
    "Perhaps the easiest way to develop intuition about how `nn.Block` works\n",
    "is to just dive right in and implement one ourselves.\n",
    "In the following snippet, instead of relying on `nn.Sequential`,\n",
    "we just code up a Block from scratch that implements a multilayer perceptron with one hidden layer, 256 hidden nodes, and 10 outputs.\n",
    "\n",
    "Our `MLP` class below inherits the `Block` class.\n",
    "While we rely on some predefined methods in the parent class,\n",
    "we need to supply our own `__init__` and `forward` functions\n",
    "to uniquely define the behavior of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "\n",
    "class MLP(nn.Block):\n",
    "    # Declare a layer with model parameters. Here, we declare two fully connected layers\n",
    "    def __init__(self, **kwargs):\n",
    "        # Call the constructor of the MLP parent class Block to perform the necessary initialization. \n",
    "        # In this way, other function parameters can also be specified when constructing an instance, \n",
    "        # such as the model parameter, params, described in the following sections\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Dense(256, activation='relu')  # Hidden layer\n",
    "        self.output = nn.Dense(10)  # Output layer\n",
    "\n",
    "    # Define the forward computation of the model, that is, \n",
    "    # how to return the required model output based on the input x\n",
    "    def forward(self, x):\n",
    "        return self.output(self.hidden(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code may be easiest to understand by working backwards from `forward`.\n",
    "Note that the `forward` method takes as input `x`.\n",
    "The forward method first evaluates `self.hidden(x)`\n",
    "to produce the hidden representation, passing this output\n",
    "as the input to the output layer `self.output( ... )`.\n",
    "\n",
    "The constituent layers of each `MLP` must be instance-level variables.\n",
    "After all, if we instantiated two such models `net1` and `net2`\n",
    "and trained them on different data,\n",
    "we would expect them to represent two different learned models.\n",
    "\n",
    "The `__init__` method is the most natural place to instantiate the layers\n",
    "that we subsequently invoke on each call to the `forward` method.\n",
    "Note that before getting on with the interesting parts,\n",
    "our customized `__init__` method must invoke the parent class's\n",
    "init method: `super(MLP, self).__init__(**kwargs)`\n",
    "to save us from reimplementing boilerplate code applicable to most Blocks.\n",
    "Then, all that is left is to instantiate our two `Dense` layers,\n",
    "assigning them to `self.hidden` and `self.output`, respectively.\n",
    "Again note that when dealing with standard functionality like this,\n",
    "we do not have to worry about backpropagation,\n",
    "since the `backward` method is generated for us automatically.\n",
    "The same goes for the `initialize` method.\n",
    "Let's try this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03989594, -0.1041471 ,  0.06799038,  0.05245074,  0.02526059,\n",
       "        -0.00640342,  0.04182098, -0.01665319, -0.02067346, -0.07863817],\n",
       "       [-0.03612847, -0.07210436,  0.09159479,  0.07890771,  0.02494172,\n",
       "        -0.01028665,  0.01732428, -0.02843242,  0.03772651, -0.06671704]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net.initialize()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we argued earlier, the primary virtue of the `Block` abstraction\n",
    "is its versatility.\n",
    "We can subclass `Block` to create layers\n",
    "(such as the `Dense` class provided by Gluon),\n",
    "entire models (such as the `MLP` class implemented above),\n",
    "or various components of intermediate complexity,\n",
    "a pattern that we will lean on heavily throughout\n",
    "the next chapters on convolutinoal neural networks.\n",
    "\n",
    "\n",
    "## The Sequential Block\n",
    "\n",
    "As we described earlier, the `Sequential` class itself\n",
    "is also just a subclass of `Block`,\n",
    "designed specifically for daisy-chaining other Blocks together.\n",
    "All we need to do to implement our own `MySequential` block\n",
    "is to define a few convenience functions:\n",
    "1. An `add` method for appending Blocks one by one to a list.\n",
    "2. A `forward` method to pass inputs through the chain of Blocks\n",
    "(in the order of addition).\n",
    "\n",
    "The following `MySequential` class delivers the same functionality\n",
    "as Gluon's default Sequential class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "class MySequential(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MySequential, self).__init__(**kwargs)\n",
    "\n",
    "    def add(self, block):\n",
    "        # Here, block is an instance of a Block subclass, and we assume it has a unique name.\n",
    "        # We save it in the member variable _children of the Block class, and its type is OrderedDict. \n",
    "        # When the MySequential instance calls the initialize function, \n",
    "        # the system automatically initializes all members of _children\n",
    "        self._children[block.name] = block\n",
    "\n",
    "    def forward(self, x):\n",
    "        # OrderedDict guarantees that members will be traversed in the order they were added\n",
    "        for block in self._children.values():\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core is the `add` method. It adds any block to the ordered dictionary of children. These are then executed in sequence when forward propagation is invoked. Let's see what the MLP looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07645682, -0.01130233,  0.04952145, -0.04651389, -0.04131573,\n",
       "        -0.05884133, -0.0621381 ,  0.01311472, -0.01379425, -0.02514282],\n",
       "       [-0.05124625,  0.00711231, -0.00155935, -0.07555379, -0.06675334,\n",
       "        -0.01762914,  0.00589084,  0.01447191, -0.04330775,  0.03317726]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it can be observed that the use of the `MySequential` class is no different from the use of the Sequential class described in `sec_mlp_gluon`.\n",
    "\n",
    "\n",
    "## Blocks with Code\n",
    "\n",
    "Although the Sequential class can make model construction easier, and you do not need to define the `forward` method, directly inheriting the Block class can greatly expand the flexibility of model construction. In particular, we will use Python's control flow within the forward method. While we are at it, we need to introduce another concept, that of the *constant* parameter. These are parameters that are not used when invoking backprop. This sounds very abstract but here's what is really going on. Assume that we have some function\n",
    "\n",
    "$$f(\\mathbf{x},\\mathbf{w}) = 3 \\cdot \\mathbf{w}^\\top \\mathbf{x}.$$\n",
    "\n",
    "In this case 3 is a constant parameter. We could change 3 to something else, say $c$ via\n",
    "\n",
    "$$f(\\mathbf{x},\\mathbf{w}) = c \\cdot \\mathbf{w}^\\top \\mathbf{x}.$$\n",
    "\n",
    "Nothing has really changed, except that we can adjust the value of $c$. It is still a constant as far as $\\mathbf{w}$ and $\\mathbf{x}$ are concerned. However, since Gluon does not know about this beforehand, it is worth while to give it a hand (this makes the code go faster, too, since we are not sending the Gluon engine on a wild goose chase after a parameter that does not change). `get_constant` is the method that can be used to accomplish this. Let's see what this looks like in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "class FancyMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FancyMLP, self).__init__(**kwargs)\n",
    "        # Random weight parameters created with the get_constant are not iterated during training \n",
    "        # (i.e., constant parameters)\n",
    "        self.rand_weight = self.params.get_constant(\n",
    "            'rand_weight', np.random.uniform(size=(20, 20)))\n",
    "        self.dense = nn.Dense(20, activation='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        # Use the constant parameters created, as well as the relu and dot functions\n",
    "        x = npx.relu(np.dot(x, self.rand_weight.data()) + 1)\n",
    "        # Reuse the fully connected layer. \n",
    "        # This is equivalent to sharing parameters with two fully connected layers\n",
    "        x = self.dense(x)\n",
    "        # Here in Control flow, we need to call asscalar to return the scalar for comparison\n",
    "        while np.abs(x).sum() > 1:\n",
    "            x /= 2\n",
    "        if np.abs(x).sum() < 0.8:\n",
    "            x *= 10\n",
    "        return x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this `FancyMLP` model, we used constant weight `Rand_weight` (note that it is not a model parameter), performed a matrix multiplication operation (`np.dot<`), and reused the *same* `Dense` layer. Note that this is very different from using two dense layers with different sets of parameters. Instead, we used the same network twice. Quite often in deep networks one also says that the parameters are *tied* when one wants to express that multiple parts of a network share the same parameters. Let's see what happens if we construct it and feed data through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(5.2637568)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FancyMLP()\n",
    "net.initialize()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no reason why we couldn't mix and match these ways of build a network. Obviously the example below resembles more a chimera, or less charitably, a [Rube Goldberg Machine](https://en.wikipedia.org/wiki/Rube_Goldberg_machine). That said, it combines examples for building a block from individual blocks, which in turn, may be blocks themselves. Furthermore, we can even combine multiple strategies inside the same forward function. To demonstrate this, here's the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.97720534)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NestMLP, self).__init__(**kwargs)\n",
    "        self.net = nn.Sequential()\n",
    "        self.net.add(nn.Dense(64, activation='relu'),\n",
    "                     nn.Dense(32, activation='relu'))\n",
    "        self.dense = nn.Dense(16, activation='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense(self.net(x))\n",
    "\n",
    "chimera = nn.Sequential()\n",
    "chimera.add(NestMLP(), nn.Dense(20), FancyMLP())\n",
    "\n",
    "chimera.initialize()\n",
    "chimera(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation\n",
    "\n",
    "The avid reader is probably starting to worry about the efficiency of this. After all, we have lots of dictionary lookups, code execution, and lots of other Pythonic things going on in what is supposed to be a high performance deep learning library. The problems of Python's [Global Interpreter Lock](https://wiki.python.org/moin/GlobalInterpreterLock) are well known. In the context of deep learning it means that we have a super fast GPU (or multiple of them) which might have to wait until a puny single CPU core running Python gets a chance to tell it what to do next. This is clearly awful and there are many ways around it. The best way to speed up Python is by avoiding it altogether.\n",
    "\n",
    "Gluon does this by allowing for Hybridization (`sec_hybridize`). In it, the Python\n",
    "interpreter executes the block the first time it is invoked. The Gluon runtime\n",
    "records what is happening and the next time around it short circuits any calls\n",
    "to Python. This can accelerate things considerably in some cases but care needs\n",
    "to be taken with control flow. We suggest that the interested reader skip\n",
    "forward to the section covering hybridization and compilation after finishing\n",
    "the current chapter.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "* Layers are blocks\n",
    "* Many layers can be a block\n",
    "* Many blocks can be a block\n",
    "* Code can be a block\n",
    "* Blocks take care of a lot of housekeeping, such as parameter initialization, backprop and related issues.\n",
    "* Sequential concatenations of layers and blocks are handled by the eponymous `Sequential` block."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
